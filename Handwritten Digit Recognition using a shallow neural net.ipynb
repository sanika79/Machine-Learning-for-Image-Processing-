{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from matplotlib import pyplot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "import MNISTtools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "xtrain,ltrain= MNISTtools.load(dataset='training', path='C:\\\\Users\\\\Sanika\\\\Desktop\\\\MLIP\\\\Assignment 1')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(784, 60000)\n",
      "(60000,)\n"
     ]
    }
   ],
   "source": [
    "print(xtrain.shape)    #shape of xtrain (N= 60000)\n",
    "print(ltrain.shape)    #shape of ltrain\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "47040000\n"
     ]
    }
   ],
   "source": [
    "print(xtrain.size)     #size of training dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    }
   ],
   "source": [
    "print(xtrain.ndim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 2 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x1c12c009940>"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPsAAAD4CAYAAAAq5pAIAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjAsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+17YcXAAAMuklEQVR4nO3dX4wdd3nG8eex8Z/IYOK1iTGO1YBj2kZIOGjlUAVo0qgoSSscqFLFrVIjRRhEIhGJC6L0grS9sRAQoaqNtGmsGESNkEIUX0QtxgIiEHKzSd3Y6QJ2wkIcb70EV8QQ4ti7by92XG2cPb9dn5lz5iTv9yOtzjnzzuy8GvnZmXN+c/xzRAjAG9+ithsA0B+EHUiCsANJEHYgCcIOJPGmfu5sqZfFcq3o5y6BVF7Wb/VKnPZctVpht329pK9IWizpXyJiZ2n95Vqhq3xdnV0CKDgQ+zvWur6Mt71Y0j9JukHSFZK22b6i298HoLfqvGffIuloRDwbEa9I+oakrc20BaBpdcK+XtJzs14fq5a9iu0dtkdtj57R6Rq7A1BHnbDP9SHAa+69jYiRiBiOiOElWlZjdwDqqBP2Y5I2zHp9qaTj9doB0Ct1wv64pE2232l7qaRbJO1tpi0ATet66C0iztq+Q9K/a2bobVdEPN1YZwAaVWucPSIelfRoQ70A6CFulwWSIOxAEoQdSIKwA0kQdiAJwg4kQdiBJAg7kARhB5Ig7EAShB1IgrADSRB2IAnCDiRB2IEkCDuQBGEHkiDsQBKEHUiCsANJEHYgCcIOJEHYgSQIO5AEYQeSIOxAEoQdSIKwA0kQdiAJwg4kUWvKZtvjkk5JmpJ0NiKGm2gKQPNqhb1ybUS80MDvAdBDXMYDSdQNe0j6tu0nbO+YawXbO2yP2h49o9M1dwegW3Uv46+OiOO2L5G0z/aPI+Kx2StExIikEUla6aGouT8AXap1Zo+I49XjpKSHJW1poikAzes67LZX2H7LueeSPizpcFONAWhWncv4tZIetn3u9/xrRPxbI10BaFzXYY+IZyW9t8FeAPQQQ29AEoQdSIKwA0kQdiAJwg4k0cQXYfB6NjN02tHijZcV6z/763XF+of+7D871ratPlDc9gt//hfF+tTYkWIdr8aZHUiCsANJEHYgCcIOJEHYgSQIO5AEYQeSYJz9DWDxuzd2rI3fvLa47Qe3dh4Hl6R/Xv9QVz0txMTUS8W6T5XruDCc2YEkCDuQBGEHkiDsQBKEHUiCsANJEHYgCcbZB8D0BzYX6yc/Vx5v/s7mBzvWVi5aXtz2od+uKtY37ftEse43TRfrP732gY61vxq7tbjtRcd+VqzjwnBmB5Ig7EAShB1IgrADSRB2IAnCDiRB2IEkGGdvwEsfu6pYv3PnnmL9gxf9sFhfveiiYv0Pvv/pjrV37Fla3HbF939crG968YliffqPryzWdW3n0vNj5e/aXy7G2Zs075nd9i7bk7YPz1o2ZHuf7SPVY/nODACtW8hl/IOSrj9v2V2S9kfEJkn7q9cABti8YY+IxySdPG/xVkm7q+e7Jd3UcF8AGtbtB3RrI2JCkqrHSzqtaHuH7VHbo2d0usvdAair55/GR8RIRAxHxPASLev17gB00G3YT9heJ0nV42RzLQHohW7DvlfS9ur5dkmPNNMOgF6Zd5zd9h5J10haY/uYpM9L2inpm7Zvk/QLSTf3sslB99Ka8t/Mfxz/k2L9718qj6MvfeTiYv1du/+jc3F6qrhtudpbi18uzw2PZs0b9ojY1qF0XcO9AOghbpcFkiDsQBKEHUiCsANJEHYgCb7i2oA1Iz8qrzBSLr+9uVb6btnf/U/X215+7zPFepvDgm9EnNmBJAg7kARhB5Ig7EAShB1IgrADSRB2IAnG2VHL+4f4755fLzizA0kQdiAJwg4kQdiBJAg7kARhB5Ig7EASjLOjp+6efF/H2vSvzp9CEL3EmR1IgrADSRB2IAnCDiRB2IEkCDuQBGEHkmCcHUWL372xWL991deK9RsO/U3H2lvPHu2qJ3Rn3jO77V22J20fnrXsHtvP2z5Y/dzY2zYB1LWQy/gHJV0/x/J7I2Jz9fNos20BaNq8YY+IxyRxXyPwOlfnA7o7bD9VXeav6rSS7R22R22PntHpGrsDUEe3Yb9P0kZJmyVNSPpSpxUjYiQihiNieImWdbk7AHV1FfaIOBERUxExLel+SVuabQtA07oKu+11s15+VNLhTusCGAzzjrPb3iPpGklrbB+T9HlJ19jeLCkkjUv6ZA97RIvGb15brK9ctLxYX3bfUJPtoIZ5wx4R2+ZY/EAPegHQQ9wuCyRB2IEkCDuQBGEHkiDsQBJ8xRVFy6/6VbF+VlPF+oqj/9uxVt4STePMDiRB2IEkCDuQBGEHkiDsQBKEHUiCsANJMM6Oove8baJY3/nCe4v1qbEjTbaDGjizA0kQdiAJwg4kQdiBJAg7kARhB5Ig7EAShB1IgrADSRB2IAnCDiRB2IEkCDuQBGEHkiDsQBJ8nz25xWtWF+tfvHRvsf7p8a3z7OGFC+wIvTLvmd32BtvftT1m+2nbn6mWD9neZ/tI9biq9+0C6NZCLuPPSvpsRPyhpPdLut32FZLukrQ/IjZJ2l+9BjCg5g17RExExJPV81OSxiStl7RV0u5qtd2SbupVkwDqu6AP6GxfJulKSQckrY2ICWnmD4KkSzpss8P2qO3RMzpdr1sAXVtw2G2/WdJDku6MiBcXul1EjETEcEQML9GybnoE0IAFhd32Es0E/esR8a1q8Qnb66r6OkmTvWkRQBPmHXqzbUkPSBqLiC/PKu2VtF3SzurxkZ50iJ6auOX3i/XViy4q1p+7f1OxfjFDbwNjIePsV0u6VdIh2werZXdrJuTftH2bpF9Iurk3LQJowrxhj4gfSHKH8nXNtgOgV7hdFkiCsANJEHYgCcIOJEHYgST4imtyb/3I8Vrbr/z5yw11gl7jzA4kQdiBJAg7kARhB5Ig7EAShB1IgrADSTDOjqJnzv6uWF9y/NfF+lSTzaAWzuxAEoQdSIKwA0kQdiAJwg4kQdiBJAg7kATj7MndcunjxfrB0+8o1qeOPNtkO+ghzuxAEoQdSIKwA0kQdiAJwg4kQdiBJAg7kMRC5mffIOmrkt4uaVrSSER8xfY9kj4h6ZfVqndHxKO9ahTdGf+HPyrWP3XxfcX65d/7eLG+UQeLdQyOhdxUc1bSZyPiSdtvkfSE7X1V7d6I+GLv2gPQlIXMzz4haaJ6fsr2mKT1vW4MQLMu6D277cskXSnpQLXoDttP2d5le1WHbXbYHrU9ekanazULoHsLDrvtN0t6SNKdEfGipPskbZS0WTNn/i/NtV1EjETEcEQML9GyBloG0I0Fhd32Es0E/esR8S1JiogTETEVEdOS7pe0pXdtAqhr3rDbtqQHJI1FxJdnLV83a7WPSjrcfHsAmrKQT+OvlnSrpEO2z42z3C1pm+3NkkLSuKRP9qRD1HJmaLrW9msf5q3XG8VCPo3/gSTPUWJMHXgd4Q46IAnCDiRB2IEkCDuQBGEHkiDsQBKOiL7tbKWH4ipf17f9AdkciP16MU7ONVTOmR3IgrADSRB2IAnCDiRB2IEkCDuQBGEHkujrOLvtX0r6+axFayS90LcGLsyg9jaofUn01q0me/u9iHjbXIW+hv01O7dHI2K4tQYKBrW3Qe1Lordu9as3LuOBJAg7kETbYR9pef8lg9rboPYl0Vu3+tJbq+/ZAfRP22d2AH1C2IEkWgm77ett/8T2Udt3tdFDJ7bHbR+yfdD2aMu97LI9afvwrGVDtvfZPlI9zjnHXku93WP7+erYHbR9Y0u9bbD9Xdtjtp+2/ZlqeavHrtBXX45b39+z214s6aeS/lTSMUmPS9oWEf/d10Y6sD0uaTgiWr8Bw/aHJP1G0lcj4j3Vsi9IOhkRO6s/lKsi4nMD0ts9kn7T9jTe1WxF62ZPMy7pJkkfV4vHrtDXX6oPx62NM/sWSUcj4tmIeEXSNyRtbaGPgRcRj0k6ed7irZJ2V893a+YfS9916G0gRMRERDxZPT8l6dw0460eu0JffdFG2NdLem7W62MarPneQ9K3bT9he0fbzcxhbURMSDP/eCRd0nI/55t3Gu9+Om+a8YE5dt1Mf15XG2Gf6//HGqTxv6sj4n2SbpB0e3W5ioVZ0DTe/TLHNOMDodvpz+tqI+zHJG2Y9fpSScdb6GNOEXG8epyU9LAGbyrqE+dm0K0eJ1vu5/8N0jTec00zrgE4dm1Of95G2B+XtMn2O20vlXSLpL0t9PEatldUH5zI9gpJH9bgTUW9V9L26vl2SY+02MurDMo03p2mGVfLx6716c8jou8/km7UzCfyz0j62zZ66NDXuyT9V/XzdNu9Sdqjmcu6M5q5IrpN0mpJ+yUdqR6HBqi3r0k6JOkpzQRrXUu9fUAzbw2fknSw+rmx7WNX6Ksvx43bZYEkuIMOSIKwA0kQdiAJwg4kQdiBJAg7kARhB5L4P3ITwZ4ltyGMAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "\n",
    "#print(xtrain[:,42])     #print the image array present at 42nd position \n",
    "\n",
    "a = np.array(xtrain[:,42])\n",
    "\n",
    "    \n",
    "b = np.reshape(a,(28,28))\n",
    "\n",
    "\n",
    "\n",
    "plt.imshow(b)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "a = np.min(xtrain)    #min value of xtrain\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "255\n"
     ]
    }
   ],
   "source": [
    "b = np.max(xtrain)  #max value of xtrain \n",
    "print(b)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "255\n"
     ]
    }
   ],
   "source": [
    "full_range = b-a \n",
    "print(full_range)     #range of xtrain\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "t = type(xtrain)       #type of xtrain\n",
    "print(t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 4\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n",
      "0.0\n",
      "2.0\n",
      "1.0\n",
      "-1.0\n"
     ]
    }
   ],
   "source": [
    "def normalize_MNIST_images(xtrain):\n",
    "    \n",
    "    \n",
    "    xtrain = xtrain.astype(np.float32)    #convert to float\n",
    "    xtrain = xtrain / 255                 #divide by maximum value\n",
    "     \n",
    "    m = np.max(xtrain)                  #maximum value should be 1\n",
    "    print(m)\n",
    "    \n",
    "    n = np.min(xtrain)                  #minimum value should be 0\n",
    "    print(n)\n",
    "    \n",
    "    xtrain = 2*xtrain\n",
    "    \n",
    "    c = np.max(xtrain)                #maximum value should be 2\n",
    "    print(c)\n",
    "    \n",
    "    xtrain = xtrain - 1\n",
    "    \n",
    "    d = np.max(xtrain)                  #maximum value should be 1\n",
    "    print(d)\n",
    "    \n",
    "    b = np.min(xtrain)                  #minimum value should be -1\n",
    "    print(b)\n",
    "    \n",
    "    return xtrain\n",
    "xtrain = normalize_MNIST_images(xtrain)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 1. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 1.]\n",
      " [0. 0. 0. ... 0. 0. 0.]]\n",
      "[0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      "7\n"
     ]
    }
   ],
   "source": [
    "def label2onehot(lbl):\n",
    "    \n",
    "    d = np.zeros((lbl.max() + 1, lbl.size))    #10*60000 array created\n",
    "    d[lbl, np.arange(lbl.size)] = 1         #array indexing\n",
    "    return d\n",
    "dtrain = label2onehot(ltrain)\n",
    "print(dtrain)\n",
    "\n",
    "print(dtrain[:,42])\n",
    "print(ltrain[42])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5 0 4 ... 5 6 8]\n"
     ]
    }
   ],
   "source": [
    "def onehot2label(d):\n",
    "    \n",
    "    lbl = d.argmax(axis=0)\n",
    "    return lbl\n",
    "\n",
    "ltrain == onehot2label(dtrain)\n",
    "\n",
    "print(ltrain)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(a) :\n",
    "#     print(\"Max = \",a.max(axis = 0))\n",
    "#     print(\"1st sample\", a[:,0])\n",
    "    a=a-a.max(axis=0)\n",
    "    g = np.exp(a)\n",
    "    sm = g/np.sum(g, axis=0)\n",
    "#     print(\"Minimum activation = \",np.min(sm))\n",
    "    return sm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](IMG_20191016_220145(1).jpg) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](IMG_20191016_220214(1).jpg) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](IMG_20191016_220222(1).jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 9 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](IMG_20191016_220231(1).jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](IMG_20191016_220239(1).jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 12 (relu) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu(a1):\n",
    "    r = np.maximum(a1 , 0)\n",
    "    return r\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 13"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "784\n",
      "10\n"
     ]
    }
   ],
   "source": [
    "def init_shallow(Ni, Nh, No):\n",
    "    b1 = np.random.randn(Nh, 1) / np.sqrt((Ni+1.)/2.)\n",
    "    W1 = np.random.randn(Nh, Ni) / np.sqrt((Ni+1.)/2.)\n",
    "    b2 = np.random.randn(No, 1) / np.sqrt((Nh+1.)/2)\n",
    "    W2 = np.random.randn(No, Nh) / np.sqrt((Nh+1.)/2)\n",
    "    \n",
    "\n",
    "    return W1, b1, W2, b2\n",
    "\n",
    "Ni = xtrain.shape[0]\n",
    "print(Ni)\n",
    "\n",
    "Nh = 64\n",
    "\n",
    "No = dtrain.shape[0]\n",
    "print(No)\n",
    "\n",
    "netinit = init_shallow(Ni, Nh, No)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 14"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forwardprop_shallow(x, net):\n",
    "    W1 = net[0]\n",
    "    b1 = net[1]\n",
    "\n",
    "    W2 = net[2]\n",
    "    b2 = net[3]\n",
    "    \n",
    "    a1 = W1.dot(x) + b1\n",
    "    \n",
    "    h1 = relu(a1)\n",
    "    \n",
    "    a2 = W2.dot(h1) + b2\n",
    "    \n",
    "    y = softmax(a2)\n",
    "    \n",
    "    return y\n",
    "\n",
    "\n",
    "yinit = forwardprop_shallow(xtrain, netinit)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.826977312076303 should be around .26\n"
     ]
    }
   ],
   "source": [
    "def eval_loss(y, d):\n",
    "    e1 = - np.sum(np.log(y)*(d))                      #cross entropy loss\n",
    "    return e1/y.shape[1]\n",
    "    \n",
    "print(eval_loss(yinit, dtrain), 'should be around .26')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "93.435\n"
     ]
    }
   ],
   "source": [
    "def eval_perfs(y, lbl):\n",
    "    \n",
    "    yinit = onehot2label(y)\n",
    "    return 100.0 - np.count_nonzero(yinit == lbl)*100.0/ltrain.shape[0]      #perecentage of misclasified samples\n",
    "    \n",
    "print(eval_perfs(yinit, ltrain))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "60000"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ltrain.shape[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.count_nonzero(np.array([1,2,3])==np.array([4,2,6]))      #trial example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([False,  True, False])"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array([1,2,3])==np.array([4,2,6])                       #trial example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 10 (softmaxp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmaxp(a, e):\n",
    "    \n",
    "    p1 = np.multiply(softmax(a),e)\n",
    "    p2 = np.multiply(p1.sum(axis=0),softmax(a))      #column wise summation\n",
    "    p3 = p1-p2                                       #final value of delta\n",
    "    \n",
    "    return p3\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 12 ( relup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relup(a, e):\n",
    "    \n",
    "    l1 = relu(a)\n",
    "    \n",
    "    l1[l1 > 0] = 1    #slope of all elements greater than zero will be one since its the line y=x\n",
    "    \n",
    "    l2 = np.multiply(l1,e)\n",
    "    \n",
    "    return l2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5.172886967827633e-07 should be smaller than 1e-6\n"
     ]
    }
   ],
   "source": [
    "eps = 1e-6                          # finite difference step\n",
    "a = np.random.randn(10, 200)          # random inputs\n",
    "e = np.random.randn(10, 200)          # random directions\n",
    "\n",
    "diff = softmaxp(a, e)\n",
    "\n",
    "diff_approx = (softmax(a + eps*e)- softmax(a))/ eps\n",
    "\n",
    "rel_error = np.abs(diff - diff_approx).mean() / np.abs(diff_approx).mean()\n",
    "print(rel_error, 'should be smaller than 1e-6')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 17"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_shallow(x, d, net, gamma=.05):\n",
    "    \n",
    "    W1 = net[0]\n",
    "    b1 = net[1]\n",
    "    W2 = net[2]\n",
    "    b2 = net[3]\n",
    "    \n",
    "    Ni = W1.shape[1]\n",
    "    Nh = W1.shape[0]\n",
    "    No = W2.shape[0]\n",
    "    \n",
    "    gamma = gamma / x.shape[1]   # normalized by the training dataset size\n",
    "\n",
    "    a1 = W1.dot(x) + b1\n",
    "    \n",
    "    h1 = relu(a1)\n",
    "    \n",
    "    a2 = W2.dot(h1) + b2\n",
    "    \n",
    "    y = softmax(a2)\n",
    "    \n",
    "    e = -d/y\n",
    "    \n",
    "    delta2 =  softmaxp( a2 ,  e)\n",
    "    delta1 = relup ( a1 , W2 .T. dot ( delta2 ))\n",
    "    \n",
    "    # gradient update\n",
    "        \n",
    "    W2 = W2 - gamma * delta2 . dot ( h1 .T)\n",
    "    #print(W2)\n",
    "    W1 = W1 - gamma * delta1 . dot (x .T)\n",
    "    #print(W1)\n",
    "    b2 = b2 - gamma * delta2 .sum( axis =1 , keepdims = True )\n",
    "    b1 = b1 - gamma * delta1 .sum( axis =1 , keepdims = True )\n",
    "\n",
    "    return W1, b1, W2, b2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 18"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backprop_shallow(x, d, net, T, gamma=.05):\n",
    "    lbl = onehot2label(d)\n",
    "    while (T>0):\n",
    "        \n",
    "        #print(net)\n",
    "        net = update_shallow(x, d, net)\n",
    "        \n",
    "        #print(net)\n",
    "        y = forwardprop_shallow(x,net)\n",
    "        \n",
    "        \n",
    "        loss = eval_loss(y,d)\n",
    "        \n",
    "        print(\"Loss:\" ,loss)\n",
    "        \n",
    "        perfs = eval_perfs(y,lbl)\n",
    "        \n",
    "        print(\"Performance :\" ,perfs)\n",
    "        \n",
    "        T = T - 1\n",
    "        \n",
    "    return net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 2.425741842792139\n",
      "Performance : 82.11666666666667\n",
      "Loss: 2.2706456891395863\n",
      "Performance : 78.20666666666666\n",
      "Loss: 2.0392056790628104\n",
      "Performance : 69.86166666666666\n",
      "Loss: 1.92374225681241\n",
      "Performance : 67.425\n",
      "Loss: 1.8579724122589942\n",
      "Performance : 61.93\n"
     ]
    }
   ],
   "source": [
    "nettrain = backprop_shallow(xtrain, dtrain, netinit, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 2.425741842792139\n",
      "Performance : 82.11666666666667\n",
      "Loss: 2.2706456891395863\n",
      "Performance : 78.20666666666666\n",
      "Loss: 2.0392056790628104\n",
      "Performance : 69.86166666666666\n",
      "Loss: 1.92374225681241\n",
      "Performance : 67.425\n",
      "Loss: 1.8579724122589942\n",
      "Performance : 61.93\n",
      "Loss: 1.814169049771184\n",
      "Performance : 67.50833333333333\n",
      "Loss: 1.8166375800198205\n",
      "Performance : 65.44666666666666\n",
      "Loss: 1.7705033468220113\n",
      "Performance : 65.87\n",
      "Loss: 1.6934883446274482\n",
      "Performance : 58.775\n",
      "Loss: 1.5525653017420153\n",
      "Performance : 50.72666666666667\n",
      "Loss: 1.488997131678437\n",
      "Performance : 49.85333333333333\n",
      "Loss: 1.4843069587387603\n",
      "Performance : 46.04833333333333\n",
      "Loss: 1.380619696787515\n",
      "Performance : 44.501666666666665\n",
      "Loss: 1.4070022337188859\n",
      "Performance : 43.96333333333333\n",
      "Loss: 1.292144873481806\n",
      "Performance : 42.08166666666666\n",
      "Loss: 1.3440313074252943\n",
      "Performance : 43.585\n",
      "Loss: 1.2312463343828872\n",
      "Performance : 39.13333333333333\n",
      "Loss: 1.236308617979657\n",
      "Performance : 41.22833333333333\n",
      "Loss: 1.2049613608299021\n",
      "Performance : 36.405\n",
      "Loss: 1.1372191745905358\n",
      "Performance : 36.66166666666667\n",
      "Loss: 1.109894644721808\n",
      "Performance : 31.926666666666662\n",
      "Loss: 1.047310353040293\n",
      "Performance : 32.91333333333333\n",
      "Loss: 1.0625238831329924\n",
      "Performance : 30.314999999999998\n",
      "Loss: 1.0358121838378926\n",
      "Performance : 33.53\n",
      "Loss: 1.0754717087554264\n",
      "Performance : 31.010000000000005\n",
      "Loss: 1.0132875413836349\n",
      "Performance : 33.123333333333335\n",
      "Loss: 1.0026425991606158\n",
      "Performance : 29.788333333333327\n",
      "Loss: 0.9190058148212357\n",
      "Performance : 29.028333333333336\n",
      "Loss: 0.8960942235161035\n",
      "Performance : 26.564999999999998\n",
      "Loss: 0.8598828240120916\n",
      "Performance : 26.601666666666674\n",
      "Loss: 0.8469608184141956\n",
      "Performance : 25.61333333333333\n",
      "Loss: 0.8330379012703459\n",
      "Performance : 26.010000000000005\n",
      "Loss: 0.8386347001651139\n",
      "Performance : 26.846666666666664\n",
      "Loss: 0.8296830654882458\n",
      "Performance : 26.611666666666665\n",
      "Loss: 0.8434242133156543\n",
      "Performance : 27.86333333333333\n",
      "Loss: 0.832853512375888\n",
      "Performance : 26.86666666666666\n",
      "Loss: 0.8669185966103683\n",
      "Performance : 28.705\n",
      "Loss: 0.8490782736262011\n",
      "Performance : 27.906666666666666\n",
      "Loss: 0.9140526034808477\n",
      "Performance : 29.441666666666663\n",
      "Loss: 0.8274313413466667\n",
      "Performance : 27.004999999999995\n",
      "Loss: 0.7916737696600736\n",
      "Performance : 25.446666666666673\n",
      "Loss: 0.7540825382754768\n",
      "Performance : 23.233333333333334\n",
      "Loss: 0.7546959383165008\n",
      "Performance : 23.72333333333333\n",
      "Loss: 0.7143101823940146\n",
      "Performance : 21.476666666666674\n",
      "Loss: 0.7159020553673834\n",
      "Performance : 22.186666666666667\n",
      "Loss: 0.6865649162369117\n",
      "Performance : 20.370000000000005\n",
      "Loss: 0.6895847119622933\n",
      "Performance : 21.251666666666665\n",
      "Loss: 0.6659786867492306\n",
      "Performance : 19.576666666666668\n",
      "Loss: 0.6686089812904823\n",
      "Performance : 20.418333333333337\n",
      "Loss: 0.648992041938712\n",
      "Performance : 18.91666666666667\n",
      "Loss: 0.6502374986357913\n",
      "Performance : 19.721666666666664\n",
      "Loss: 0.6331555374158436\n",
      "Performance : 18.33333333333333\n",
      "Loss: 0.6327619462366085\n",
      "Performance : 19.056666666666672\n",
      "Loss: 0.6175454363681367\n",
      "Performance : 17.74333333333334\n",
      "Loss: 0.615774103092122\n",
      "Performance : 18.38166666666666\n",
      "Loss: 0.6020808768543944\n",
      "Performance : 17.226666666666674\n",
      "Loss: 0.5992210064275736\n",
      "Performance : 17.659999999999997\n",
      "Loss: 0.5873535412222575\n",
      "Performance : 16.650000000000006\n",
      "Loss: 0.5840130253665398\n",
      "Performance : 17.034999999999997\n",
      "Loss: 0.5737261997185881\n",
      "Performance : 16.165000000000006\n",
      "Loss: 0.5701172192310091\n",
      "Performance : 16.471666666666664\n",
      "Loss: 0.5613476205949092\n",
      "Performance : 15.75833333333334\n",
      "Loss: 0.557611185271754\n",
      "Performance : 16.025000000000006\n",
      "Loss: 0.5500857773651143\n",
      "Performance : 15.36333333333333\n",
      "Loss: 0.5463133640511769\n",
      "Performance : 15.65166666666667\n",
      "Loss: 0.5398626994930017\n",
      "Performance : 15.045000000000002\n",
      "Loss: 0.5362235827515139\n",
      "Performance : 15.213333333333338\n",
      "Loss: 0.5306349758457181\n",
      "Performance : 14.75\n",
      "Loss: 0.5271615675493814\n",
      "Performance : 14.941666666666663\n",
      "Loss: 0.5223041523427991\n",
      "Performance : 14.451666666666668\n",
      "Loss: 0.5189695467970308\n",
      "Performance : 14.659999999999997\n",
      "Loss: 0.5146890133129225\n",
      "Performance : 14.196666666666673\n",
      "Loss: 0.5115035489906351\n",
      "Performance : 14.398333333333326\n",
      "Loss: 0.5076783387245973\n",
      "Performance : 14.00333333333333\n",
      "Loss: 0.5046445897098496\n",
      "Performance : 14.148333333333326\n",
      "Loss: 0.5012042626846288\n",
      "Performance : 13.781666666666666\n",
      "Loss: 0.4983413888133907\n",
      "Performance : 13.953333333333333\n",
      "Loss: 0.49522137230415975\n",
      "Performance : 13.591666666666669\n",
      "Loss: 0.49252775828063505\n",
      "Performance : 13.75666666666666\n",
      "Loss: 0.4896764369420961\n",
      "Performance : 13.430000000000007\n",
      "Loss: 0.48712919514380637\n",
      "Performance : 13.569999999999993\n",
      "Loss: 0.4844951628253762\n",
      "Performance : 13.275000000000006\n",
      "Loss: 0.48209532246979653\n",
      "Performance : 13.39\n",
      "Loss: 0.4796617890549499\n",
      "Performance : 13.13666666666667\n",
      "Loss: 0.4773957259681562\n",
      "Performance : 13.25333333333333\n",
      "Loss: 0.4751172184813001\n",
      "Performance : 13.048333333333332\n",
      "Loss: 0.4729905212418518\n",
      "Performance : 13.168333333333337\n",
      "Loss: 0.47086253812954915\n",
      "Performance : 12.954999999999998\n",
      "Loss: 0.46885645496224854\n",
      "Performance : 13.038333333333327\n",
      "Loss: 0.4668557054040883\n",
      "Performance : 12.86833333333334\n",
      "Loss: 0.464968868276876\n",
      "Performance : 12.935000000000002\n",
      "Loss: 0.4630942695052381\n",
      "Performance : 12.75833333333334\n",
      "Loss: 0.4613312853322285\n",
      "Performance : 12.851666666666674\n",
      "Loss: 0.4595817507845925\n",
      "Performance : 12.648333333333326\n",
      "Loss: 0.4579545794444769\n",
      "Performance : 12.791666666666671\n",
      "Loss: 0.4563351418315646\n",
      "Performance : 12.563333333333333\n",
      "Loss: 0.4548501566213403\n",
      "Performance : 12.719999999999999\n",
      "Loss: 0.45338267222667794\n",
      "Performance : 12.498333333333335\n",
      "Loss: 0.4520606884991818\n",
      "Performance : 12.688333333333333\n",
      "Loss: 0.4507620555098389\n",
      "Performance : 12.454999999999998\n"
     ]
    }
   ],
   "source": [
    "nettrain = backprop_shallow(xtrain, dtrain, netinit, 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 19"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n",
      "0.0\n",
      "2.0\n",
      "1.0\n",
      "-1.0\n"
     ]
    }
   ],
   "source": [
    "xtest,ltest= MNISTtools.load(dataset='testing', path='C:\\\\Users\\\\Sanika\\\\Desktop\\\\MLIP\\\\Assignment 1')\n",
    "\n",
    "xtest = normalize_MNIST_images(xtest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(784, 10000)\n",
      "(10000,)\n"
     ]
    }
   ],
   "source": [
    "print(xtest.shape)\n",
    "print(ltest.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7840000\n"
     ]
    }
   ],
   "source": [
    "print(xtest.size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.4291869284630172\n",
      "Performance : 85.28999999999999\n"
     ]
    }
   ],
   "source": [
    "dtest = label2onehot(ltest)\n",
    "ytest = forwardprop_shallow(xtest, nettrain)\n",
    "loss = eval_loss(ytest,dtest)\n",
    "print(\"Loss:\" ,loss)\n",
    "perfs = eval_perfs(ytest,ltest)\n",
    "print(\"Performance :\" ,perfs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backprop_minibatch_shallow(x, d, net, T, B=100, gamma=.05):\n",
    "    \n",
    "    N = x.shape[1]\n",
    "\n",
    "    NB = int((N+B-1)/B)            #no of samples + batch size \n",
    "\n",
    "    lbl = onehot2label(d)\n",
    "    for t in range(T):\n",
    "        shuffled_indices = np.random.permutation(range(N))\n",
    "        for l in range(NB):\n",
    "            start = B*l\n",
    "            if B * (l + 1)> N:\n",
    "                end = N\n",
    "            else:\n",
    "                end = B *(l+1)\n",
    "            minibatch_indices = shuffled_indices[start:end]\n",
    "            net = update_shallow(x[:,minibatch_indices], d[:,minibatch_indices], net)\n",
    "        \n",
    "\n",
    "        y = forwardprop_shallow(x, net)\n",
    "\n",
    "        loss = eval_loss(y,d)\n",
    "\n",
    "        print(\"Loss:\" ,loss)\n",
    "\n",
    "        perfs = eval_perfs(y,lbl)\n",
    "\n",
    "        print(\"Performance :\" ,perfs)\n",
    "\n",
    "    return net\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.2711271227071919\n",
      "Performance : 7.846666666666664\n",
      "Loss: 0.22923723774828036\n",
      "Performance : 6.644999999999996\n",
      "Loss: 0.18004849543880985\n",
      "Performance : 5.266666666666666\n",
      "Loss: 0.1535280260856061\n",
      "Performance : 4.38333333333334\n",
      "Loss: 0.1489401660930424\n",
      "Performance : 4.204999999999998\n"
     ]
    }
   ],
   "source": [
    "netbatch =  backprop_minibatch_shallow(xtrain, dtrain, netinit, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.1616315594934502\n",
      "Performance : 84.09666666666666\n"
     ]
    }
   ],
   "source": [
    "dtest = label2onehot(ltest)\n",
    "ytest = forwardprop_shallow(xtest, netbatch)\n",
    "loss = eval_loss(ytest,dtest)\n",
    "print(\"Loss:\" ,loss)\n",
    "perfs = eval_perfs(ytest,ltest)\n",
    "print(\"Performance :\" ,perfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
